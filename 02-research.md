---
layout: page
title: Research
permalink: /research/
---

## Topics

### Audiovisual speech enhancement

AVCOGHEAR demos etc.

### Intelligibility modelling

#### Microscopic intelligibility modelling evaluation framework

#### Consistent confusions

   <img style="max-width: 200px; filter: invert(100%); float: right; margin: 1em; overflow: auto;" src="{{ site.baseurl }}/assets/consistent_confusion.png">

[The English Consistent Confusion Corpus](http://spandh.dcs.shef.ac.uk/ECCC/) is a large-scale collection of noise induced British English speech misperceptions. These misperceptions have been elicited by asking listeners to transcribe English words mixed with complex noise backgrounds.

The corpus has been distilled from over 300,000 listener responses and includes responses to over 9,000 individual noisy speech tokens. Of these, a subset of over 3,000 tokens induce 'consistent confusions', i.e. tokens that are misheard in the same way by a significant number of listeners.

### Vocal interactivity in-and-between humans, animals and robots

   <img style="max-width: 200px; float: right; margin: 1em; overflow: auto;" src="{{ site.baseurl }}/assets/vihar_schema.png">

Almost all animals exploit vocal signals for a range of ecologically-motivated purposes: from detecting predators/prey and marking territory, to expressing emotions, establishing social relations and sharing information.  Whether it is a bird raising an alarm, a whale calling to potential partners, a dog responding to human commands, a parent reading a story with a child, or a businessperson accessing stock prices using Siri on an iPhone, vocalisation provides a valuable communications channel through which behaviour may be coordinated and controlled, and information may be distributed and acquired.  Indeed, the ubiquity of vocal interaction has led to research across an extremely diverse array of fields, from assessing animal welfare, to understanding the precursors of human language, to developing voice-based human-machine interaction.
This work is dedicated to the interdisciplinary study of vocal interactivity in-and-between humans, animal and robots.


### Audio source separation
Demos of lead voice separation
Demos of bass, drums, ...

### Unsupervised incremental clustering
Demos of incremental clustering



## Projects
 - [INSPIRE-ITN](http://www.inspire-itn.eu/) Investigating Speech Processing In Realistic Environments
 - [EmCAP](http://emcap.iua.upf.edu/) Emergent Cognition through Active Perception
 - [CloudCAST](http://cloudcast.rcweb.dcs.shef.ac.uk/) 
 - Audio Object Modelling in Mixtures joint research project with Yamaha Corp.
 - Minus-One Technology (audio source separation) joint research project with Yamaha Corp.


## Activities
 - Organiser of the [Intelligibility under the Microscope](http://spandh.dcs.shef.ac.uk/2016_is_microintelligibility/) IS2016 special session
 - Co-organiser of the [CHiME3](http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/) & [CHiME4](http://spandh.dcs.shef.ac.uk/chime_challenge/) challenges
 - Co-organiser of the [CHiME 2016 workshop](http://spandh.dcs.shef.ac.uk/chime_workshop/)
 - Coordinator of the [VIHAR Dagstuhl seminar 2016](http://www.dagstuhl.de/16442)
 - Co-organiser of the [VIHAR 2017 workshop](http://vihar207.vihar.org)
